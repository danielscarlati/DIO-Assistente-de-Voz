{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "id": "initial_id"
      },
      "outputs": [],
      "source": [
        "language = \"pt\""
      ]
    },
    {
      "metadata": {
        "id": "1124759f9db6e00a"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "#Gravação de audio com Python e JS\n",
        "from IPython.display import Audio, display, Javascript\n",
        "from google.colab import output\n",
        "from base64 import b64decode\n",
        "\n",
        "RECORD = \"\"\"\n",
        "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
        "const b2text = blob => new Promise(resolve => {\n",
        "  const reader = new FileReader()\n",
        "  reader.onloadend = e => resolve(e.srcElement.result)\n",
        "  reader.readAsDataURL(blob)\n",
        "})\n",
        "var record = time => new Promise(async resolve => {\n",
        "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
        "  recorder = new MediaRecorder(stream)\n",
        "  chunks = []\n",
        "  recorder.ondataavailable = e => chunks.push(e.data)\n",
        "  recorder.start()\n",
        "  await sleep(time)\n",
        "  recorder.onstop = async ()=>{\n",
        "    blob = new Blob(chunks)\n",
        "    text = await b2text(blob)\n",
        "    resolve(text)\n",
        "  }\n",
        "  recorder.stop()\n",
        "})\n",
        "\"\"\"\n",
        "\n",
        "def record(sec=5):\n",
        "  display(Javascript(RECORD))\n",
        "  js_time_ms = sec * 1000\n",
        "  js_result = output.eval_js(f'record({js_time_ms})')\n",
        "  audio = b64decode(js_result.split(',')[1])\n",
        "  file_name = 'request_audio.wav'\n",
        "  with open(file_name, 'wb') as f:\n",
        "      f.write(audio)\n",
        "  return f\"/content/{file_name}\"\n",
        "\n",
        "print(\"Gravando...\\n\")\n",
        "record_file = record()\n",
        "display(Audio(record_file, autoplay=True))"
      ],
      "id": "1124759f9db6e00a"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/whisper.git -q"
      ],
      "metadata": {
        "id": "B0L7dwnuGMSj",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "B0L7dwnuGMSj"
    },
    {
      "cell_type": "code",
      "source": [
        "#Reconhecimento de fala Whisper\n",
        "import whisper\n",
        "\n",
        "model = whisper.load_model(\"small\")\n",
        "result = model.transcribe(record_file, fp16=False, language=language)\n",
        "transcription = result[\"text\"]\n",
        "print(transcription)"
      ],
      "metadata": {
        "id": "gtWhDIwxOS7g"
      },
      "id": "gtWhDIwxOS7g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "POOKGGTsnlA7"
      },
      "id": "POOKGGTsnlA7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = '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'"
      ],
      "metadata": {
        "id": "dXu72z77GXc1"
      },
      "id": "dXu72z77GXc1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "# Instantiate the OpenAI client. It will automatically pick up the API key from the OPENAI_API_KEY environment variable.\n",
        "client = OpenAI()\n",
        "\n",
        "# Lembrando que, a variável 'transcription' contém a transcrição do nosso áudio.\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\", # Using a common model, you can change this if needed\n",
        "    messages=[ { \"role\": \"user\", \"content\": transcription } ]\n",
        ")\n",
        "\n",
        "# Obtém a resposta gerada pelo ChatGPT\n",
        "chatgpt_response = response.choices[0].message.content\n",
        "print(chatgpt_response)"
      ],
      "metadata": {
        "id": "o1MKqMD7n2oE"
      },
      "id": "o1MKqMD7n2oE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gTTS"
      ],
      "metadata": {
        "id": "bFDcRWAt-n7l"
      },
      "id": "bFDcRWAt-n7l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sintetizando a resposta como voz\n",
        "from gtts import gTTS\n",
        "\n",
        "gtts_object = gTTS(text=chatgpt_response, lang=language, slow=False)\n",
        "response_audio = \"/content/response_audio.wav\"\n",
        "gtts_object.save(response_audio)\n",
        "display(Audio(response_audio, autoplay=True))\n"
      ],
      "metadata": {
        "id": "QY1foNIF-9Cj"
      },
      "id": "QY1foNIF-9Cj",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}